{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cupy as np\n",
    "from matplotlib import pyplot as plt\n",
    "# ----- Cord for importing .ipynb file as module -----\n",
    "import sys  \n",
    "import notebookutil as nbu\n",
    "sys.meta_path.append(nbu.NotebookFinder())\n",
    "# ----------------------------------------------------\n",
    "from activations import *\n",
    "from loss_functions import *\n",
    "from optimizer import *\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_openml\n",
    "from typing import Callable, Any, List, Dict\n",
    "\n",
    "class cnn_module:\n",
    "    def __init__(self, structure:List, init_weight_range:float):\n",
    "        self.params = {}\n",
    "        self.conv_params = {}\n",
    "        self.pool_params = {}\n",
    "        self.grads = {}\n",
    "        self.outputs = {}\n",
    "        self.activations = {}\n",
    "        self.L = len(structure)\n",
    "        for l in range(self.L):\n",
    "            if structure[l][0] == 'conv':\n",
    "                in_channel = structure[l][1]\n",
    "                out_channel = structure[l][2]\n",
    "                kernel_size = structure[l][3]\n",
    "                W = np.random.uniform(\n",
    "                    low = - init_weight_range,\n",
    "                    high = init_weight_range,\n",
    "                    size = (out_channel, in_channel, kernel_size, kernel_size)\n",
    "                )\n",
    "                b = np.zeros((1, 1, out_channel))\n",
    "                self.params['conv_W' + str(l)] = W\n",
    "                self.params['conv_b' + str(l)] = b\n",
    "                forward_l = 'forward_' + str(l)\n",
    "                backward_l = 'backward_' + str(l)\n",
    "                self.activations[forward_l] = structure[l][4]\n",
    "                self.activations[backward_l] = structure[l][5]\n",
    "                setattr(self, forward_l, self.forward_conv)\n",
    "                setattr(self, backward_l, self.backward_conv)\n",
    "        \n",
    "            elif structure[l][0] == 'pool':\n",
    "                self.pool_params['kernel_size' + str(l)] = structure[l][1]\n",
    "                forward_l = 'forward_' + str(l)\n",
    "                backward_l = 'backward_' + str(l)\n",
    "                setattr(self, forward_l, self.forward_pool)\n",
    "                setattr(self, backward_l, self.backward_pool)\n",
    "                \n",
    "            elif structure[l][0] == 'fc':\n",
    "                in_size = structure[l][1]\n",
    "                out_size = structure[l][2]\n",
    "                W = np.random.uniform(\n",
    "                    low = - init_weight_range,\n",
    "                    high = init_weight_range,\n",
    "                    size = (in_size, out_size)\n",
    "                )\n",
    "                b = np.zeros((1, out_size))\n",
    "                self.params['fc_W' + str(l)] = W\n",
    "                self.params['fc_b' + str(l)] = b\n",
    "                forward_l = 'forward_' + str(l)\n",
    "                backward_l = 'backward_' + str(l)\n",
    "                self.activations[forward_l] = structure[l][3]\n",
    "                self.activations[backward_l] = structure[l][4]\n",
    "                setattr(self, forward_l, self.forward_fc)\n",
    "                setattr(self, backward_l, self.backward_fc)\n",
    "    \n",
    "    def forward_conv(self, x:np.array, num_layer:int, slide_size=1) -> np.array:\n",
    "        W = self.params['conv_W' + str(num_layer)]\n",
    "        b = self.params['conv_b' + str(num_layer)]\n",
    "        N, in_channel, H_in, W_in = x.shape\n",
    "        out_channel, in_channel, kernel_size = W.shape[:3]\n",
    "        H_out = int((H_in - kernel_size) / slide_size) + 1\n",
    "        W_out = int((W_in - kernel_size) / slide_size) + 1\n",
    "        params = [kernel_size, slide_size, N, in_channel, H_in, W_in, out_channel, H_out, W_out]\n",
    "        self.conv_params['params' + str(num_layer)] = params\n",
    "        patch = np.zeros((N, H_out * W_out, in_channel, kernel_size, kernel_size))\n",
    "        for h in range(0, H_out, slide_size):\n",
    "            for w in range(0, W_out, slide_size):\n",
    "                hs = h\n",
    "                he = hs + kernel_size\n",
    "                ws = w\n",
    "                we = ws + kernel_size\n",
    "                patch[:, h * W_out + w, :, :, :] = x[:, :, hs:he, ws:we]\n",
    "        Z = np.tensordot(patch, W, ([2, 3, 4], [1, 2, 3])) + b\n",
    "        Z = Z.swapaxes(1, 2).reshape(N, out_channel, H_out, W_out)\n",
    "        A = self.activations['forward_' + str(num_layer)](Z)\n",
    "        self.conv_params['conv_patch' + str(num_layer)] = patch\n",
    "        self.outputs['Z' + str(num_layer)] = Z\n",
    "        self.outputs['A' + str(num_layer)] = A\n",
    "        return A\n",
    "\n",
    "    def backward_conv(self, dA:np.array, num_layer:int):\n",
    "        W = self.params['conv_W' + str(num_layer)]\n",
    "        Z = self.outputs['Z' + str(num_layer)]\n",
    "        patch = self.conv_params['conv_patch' + str(num_layer)]\n",
    "        params = self.conv_params['params' + str(num_layer)]\n",
    "        kernel_size, slide_size, N, in_channel, H_in, W_in, out_channel, H_out, W_out = params\n",
    "        dZ = dA * self.activations['backward_' + str(num_layer)](Z)\n",
    "        dZ = dZ.reshape(N, out_channel, H_out * W_out).swapaxes(1, 2)\n",
    "        dW = np.tensordot(dZ, patch, ([0, 1], [0, 1]))\n",
    "        db = np.sum(dZ, axis=(0, 1), keepdims=True)\n",
    "        # --------------------------------------------------------------\n",
    "        assert(dW.shape == self.params['conv_W' + str(num_layer)].shape)\n",
    "        assert(db.shape == self.params['conv_b' + str(num_layer)].shape)\n",
    "        # --------------------------------------------------------------        \n",
    "        self.grads['conv_W' + str(num_layer)] = dW\n",
    "        self.grads['conv_b' + str(num_layer)] = db\n",
    "        dA_ = np.tensordot(dZ, W, (2, 0))\n",
    "        dA = np.zeros((N, in_channel, H_in, W_in))\n",
    "        for h in range(0, H_out, slide_size):\n",
    "            for w in range(0, W_out, slide_size):\n",
    "                hs = h\n",
    "                he = hs + kernel_size\n",
    "                ws = w\n",
    "                we = ws + kernel_size\n",
    "                dA[:, :, hs:he, ws:we] = dA_[:, h * W_out + w, :, :, :]\n",
    "        return dA\n",
    "\n",
    "    def forward_pool(self, x:np.array, num_layer:int) -> np.array:\n",
    "        kernel_size = self.pool_params['kernel_size' + str(num_layer)]\n",
    "        slide_size = kernel_size\n",
    "        N, n_channel, H_in, W_in = x.shape\n",
    "        H_out = int((H_in - kernel_size) / slide_size) + 1\n",
    "        W_out = int((W_in - kernel_size) / slide_size) + 1\n",
    "        params = [kernel_size, slide_size, N, n_channel, H_in, W_in, H_out, W_out]\n",
    "        self.pool_params['params' + str(num_layer)] = params\n",
    "        patch = np.zeros((N, H_out * W_out, n_channel, kernel_size, kernel_size))\n",
    "        for h in range(0, H_out, slide_size):\n",
    "            for w in range(0, W_out, slide_size):\n",
    "                hs = h\n",
    "                he = hs + kernel_size\n",
    "                ws = w\n",
    "                we = ws + kernel_size\n",
    "                patch[:, h * W_out + w, :, :, :] = x[:, :, hs:he, ws:we]\n",
    "        patch = patch.swapaxes(1, 2).reshape(N, n_channel, H_out * W_out, kernel_size * kernel_size)\n",
    "        A = np.max(patch, axis=3).reshape(N, n_channel, H_out, W_out)\n",
    "        index = np.argmax(patch, axis=3).flatten()\n",
    "        self.pool_params['index' + str(num_layer)] = index\n",
    "        self.outputs['A' + str(num_layer)] = A\n",
    "        return A\n",
    "    \n",
    "    def backward_pool(self, delta:np.array, num_layer:int):\n",
    "        params = self.pool_params['params' + str(num_layer)]\n",
    "        kernel_size, slide_size, N, n_channel, H_in, W_in, H_out, W_out = params\n",
    "        index_flat = self.pool_params['index' + str(num_layer)]\n",
    "        rh, rw = int(H_in / H_out), int(W_in / W_out)\n",
    "        #--------------------------------------------------\n",
    "        assert(H_in % H_out == 0 and W_in % W_out == 0)\n",
    "        #--------------------------------------------------\n",
    "        index = np.arange(N * n_channel * H_out * W_out) * rh * rw + index_flat\n",
    "        delta_flatten = np.zeros(N * n_channel * H_in * W_in)\n",
    "        delta_flatten[index] = delta.flatten()\n",
    "        return delta_flatten.reshape(N, n_channel, H_in, W_in)\n",
    "                \n",
    "    def forward_fc(self, x:np.array, num_layer:int) -> np.array:\n",
    "        W = self.params['fc_W' + str(num_layer)]\n",
    "        b = self.params['fc_b' + str(num_layer)]\n",
    "        N = x.shape[0]\n",
    "        in_size = W.shape[0]\n",
    "        x = x.flatten().reshape(N, in_size)\n",
    "        Z = np.dot(x, W) + b\n",
    "        A = self.activations['forward_' + str(num_layer)](Z)\n",
    "        self.outputs['Z' + str(num_layer)] = Z\n",
    "        self.outputs['A' + str(num_layer)] = A\n",
    "        return A\n",
    "    \n",
    "    def backward_fc(self, dA:np.array, num_layer:int):\n",
    "        W = self.params['fc_W' + str(num_layer)]\n",
    "        b = self.params['fc_b' + str(num_layer)]\n",
    "        Z = self.outputs['Z' + str(num_layer)]\n",
    "        N = dA.shape[0]\n",
    "        A_prev = self.outputs['A' + str(num_layer - 1)].reshape(N, -1)\n",
    "        dZ = dA * self.activations['backward_' + str(num_layer)](Z)\n",
    "        dW = np.dot(A_prev.T, dZ)\n",
    "        db = np.sum(dZ, axis=0, keepdims=True)\n",
    "        # --------------------------------------------------------------\n",
    "        assert(dW.shape == self.params['fc_W' + str(num_layer)].shape)\n",
    "        assert(db.shape == self.params['fc_b' + str(num_layer)].shape)\n",
    "        # --------------------------------------------------------------        \n",
    "        self.grads['fc_W' + str(num_layer)] = dW\n",
    "        self.grads['fc_b' + str(num_layer)] = db\n",
    "        dA = np.dot(dZ, W.T)\n",
    "        return dA\n",
    "    \n",
    "    def forward_propagate(self, x:np.array):\n",
    "        self.outputs['A0'] = x\n",
    "        for l in range(self.L):\n",
    "            forward_l = 'forward_' + str(l)\n",
    "            x = getattr(self, forward_l)(x, l)\n",
    "#             print(l, x.shape)\n",
    "        return x\n",
    "    \n",
    "    def back_propagate(self, dA:np.array):\n",
    "        for l in reversed(range(self.L)):\n",
    "            backward_l = 'backward_' + str(l)\n",
    "            dA = getattr(self, backward_l)(dA, l)\n",
    "#             print(l, dA.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ir = (6 / (28 * 28 + 10)) ** 0.5\n",
    "# x = np.random.uniform(low=-ir, high=ir, size=(5, 1, 28, 28)) + 0.5\n",
    "# y = np.random.uniform(low=-ir, high=ir, size=(5, 10)) +0.5\n",
    "# structure = [\n",
    "#     ['conv', 1, 6, 5, relu, d_relu],    # (N, 1, 28, 28) -> (N, 6, 24, 24)\n",
    "#     ['pool', 2],                        # (N, 6, 24, 24) -> (N, 6, 12, 12)\n",
    "#     ['conv', 6, 16, 3, relu, d_relu],   # (N, 6, 12, 12) -> (N, 16, 10, 10)\n",
    "#     ['pool', 2],                        # (N, 16, 10, 10) -> (N, 16, 5, 5)\n",
    "#     ['fc', 16*5*5, 120, relu, d_relu],  # (N, 16, 5, 5) -> (N, 120)\n",
    "#     ['fc', 120, 84, relu, d_relu],      # (N, 120) -> (N, 84)\n",
    "#     ['fc', 84, 10, softmax, d_softmax]  # (N, 84) -> (N, 10)\n",
    "# ]\n",
    "# cnn = cnn_module(structure, ir)\n",
    "# optimizer = Adam(cnn)\n",
    "# y_pred = cnn.forward_propagate(x)\n",
    "# loss, dA = CEL(y_pred, y)\n",
    "# cnn.back_propagate(dA)\n",
    "# optimizer.update_params()\n",
    "# print(loss, dA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7000, 784) (7000, 10)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Load data set\n",
    "    mnist = fetch_openml('mnist_784', data_home='./')\n",
    "#     X = mnist.data / 255\n",
    "    X = mnist.data[:7000] / 255\n",
    "    Y = []\n",
    "#     for num in mnist.target:\n",
    "    for num in mnist.target[:7000]:\n",
    "        zeros = np.zeros(10)\n",
    "        zeros[int(num)] = 1.0\n",
    "        Y.append(zeros)\n",
    "    Y = np.array(Y)\n",
    "    print(X.shape, Y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00869291381699617\n",
      "Teach data: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Set cnn\n",
    "    init_weight_range = (6 / (28 * 28 + 10)) ** 0.5 * 0.1\n",
    "    structure = [\n",
    "        ['conv', 1, 6, 5, relu, d_relu],    # (N, 1, 28, 28) -> (N, 6, 24, 24)\n",
    "        ['pool', 2],                        # (N, 6, 24, 24) -> (N, 6, 12, 12)\n",
    "        ['conv', 6, 16, 3, relu, d_relu],   # (N, 6, 12, 12) -> (N, 16, 10, 10)\n",
    "        ['pool', 2],                        # (N, 16, 10, 10) -> (N, 16, 5, 5)\n",
    "        ['fc', 16*5*5, 120, relu, d_relu],  # (N, 16, 5, 5) -> (N, 120)\n",
    "        ['fc', 120, 84, relu, d_relu],      # (N, 120) -> (N, 84)\n",
    "        ['fc', 84, 10, softmax, d_softmax]  # (N, 84) -> (N, 10)\n",
    "    ]\n",
    "    cnn = cnn_module(structure, init_weight_range)\n",
    "    optimizer = SGD(cnn, 0.5)\n",
    "#     optimizer = Adam(cnn)\n",
    "    print(init_weight_range)\n",
    "    # Prepare train & test data\n",
    "    N = X.shape[0]\n",
    "    rate = 1 / 7\n",
    "    N_test = int(N * rate)\n",
    "    N_train = N - N_test\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=rate)\n",
    "    X_train = X_train.reshape(N_train, 1, 28, 28)\n",
    "    Y_train = Y_train.reshape(N_train, 10)\n",
    "    X_test = X_test.reshape(N_test, 1, 28, 28)\n",
    "    Y_test = Y_test.reshape(N_test, 10)\n",
    "    # Plot example of data & label\n",
    "    print('Teach data:', Y_train[0, :])\n",
    "    plt.figure()\n",
    "    plt.imshow(X_train[0, 0, :, :])\n",
    "    plt.gray()    \n",
    "    \n",
    "# x = torch.tensor(x, dtype=torch.float32)\n",
    "# y = torch.tensor(y, dtype=torch.int64)\n",
    "# y_pred = cnn.forward_propagate(x)\n",
    "# print(y_pred.shape, y_pred[0])\n",
    "# loss, dA = CEL(y_pred, y)\n",
    "# print(dA.shape, loss)\n",
    "# cnn.back_propagate(dA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1   Train Loss: 2.3016   Test Loss: 2.3040\n",
      "Epoch: 2   Train Loss: 2.3000   Test Loss: 2.3071\n",
      "Epoch: 3   Train Loss: 2.3009   Test Loss: 2.3165\n",
      "Epoch: 4   Train Loss: 2.5555   Test Loss: nan\n",
      "Epoch: 5   Train Loss: nan   Test Loss: nan\n",
      "Epoch: 6   Train Loss: nan   Test Loss: nan\n",
      "Epoch: 7   Train Loss: nan   Test Loss: nan\n",
      "Epoch: 8   Train Loss: nan   Test Loss: nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-96261c5ec360>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m             \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_propagate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCEL\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[0mcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mback_propagate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-8a05eec8a15b>\u001b[0m in \u001b[0;36mforward_propagate\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    191\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m             \u001b[0mforward_l\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'forward_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforward_l\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;31m#             print(l, x.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-8a05eec8a15b>\u001b[0m in \u001b[0;36mforward_pool\u001b[1;34m(self, x, num_layer)\u001b[0m\n\u001b[0;32m    138\u001b[0m                 \u001b[0mpatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mW_out\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mhe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mws\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mwe\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[0mpatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mswapaxes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_channel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mH_out\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mW_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m         \u001b[0mA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_channel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mH_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'index'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_layer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mamax\u001b[1;34m(a, axis, out, keepdims, initial)\u001b[0m\n\u001b[0;32m   2503\u001b[0m     \"\"\"\n\u001b[0;32m   2504\u001b[0m     return _wrapreduction(a, np.maximum, 'max', axis, None, out, keepdims=keepdims,\n\u001b[1;32m-> 2505\u001b[1;33m                           initial=initial)\n\u001b[0m\u001b[0;32m   2506\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2507\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     84\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Learning data\n",
    "    epoches = 100\n",
    "    batch_size = 500\n",
    "    Loss_train = []\n",
    "    Loss_test = []\n",
    "    train_n_batches = len(X_train) // batch_size\n",
    "    test_n_batches = len(X_test) // batch_size\n",
    "    for epoch in range(1, epoches + 1):\n",
    "        loss_train = []\n",
    "        X_train, Y_train = shuffle(X_train, Y_train)\n",
    "        for i in range(train_n_batches):\n",
    "            start = i * batch_size\n",
    "            end = start + batch_size\n",
    "            y_pred = cnn.forward_propagate(X_train[start:end])\n",
    "            loss, delta = CEL(y_pred, Y_train[start:end])\n",
    "            cnn.back_propagate(delta)\n",
    "            optimizer.update_params()\n",
    "            loss_train.append(loss)\n",
    "        loss_test = []\n",
    "        X_test, Y_test = shuffle(X_test, Y_test)\n",
    "        for i in range(test_n_batches):\n",
    "            start = i * batch_size\n",
    "            end = start + batch_size\n",
    "            y_pred = cnn.forward_propagate(X_test[start:end])\n",
    "            loss, delta = CEL(y_pred, Y_test[start:end])\n",
    "            loss_test.append(loss)\n",
    "        Loss_train.append(np.mean(loss_train))\n",
    "        Loss_test.append(np.mean(loss_test))\n",
    "        if epoch % 1 == 0 or epoch == 1:\n",
    "            print('Epoch:', epoch,\n",
    "                  '  Train Loss:', '{:.4f}'.format(Loss_train[-1]),\n",
    "                  '  Test Loss:', '{:.4f}'.format(Loss_test[-1])\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, 1)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
